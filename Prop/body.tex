
\section{Introduction}
\dpw{One of the most challenging aspects of software engineering is
optimizing and performance-debugging one's programs.  }


When writing robust software, functional correctness is only the
beginning.  High-quality code also needs to make savvy use of
resources.  In modern computer systems, there are many implementation
choices, and the best choices depend upon many factors that may be
only poorly understood by programmers.  Examples of such factors
include difficult-to-predict effects of hardware and/or software
caches, jit compilers, the changing needs of dynamic workloads, and
implementations of lazy language features.   


Overly Specialized.
Even if programmers were able to figure out at a particular point in
time how to get optimal performance on a particular machine with a
particular compiler on a particular set of workloads, such codes are
unlikely to be robust as the hardware/software/workload system
co-evolves.


Premature commitments. 
Library writers face particuarly daunting challenges as the
appropriate implementation choices may depend crucially on information
not available until the library is used.  Exposing different API calls
for different workloads, for example as Haskell's JSON parsing library
does, is a brittle and clumsy mechanism for allowing users to
customize performance.

A different approach to building robust performant systems is to allow
programmers to specify the required functionality of a given piece of
a system \textit{but not the implementation}.  
Instead, the programmer specifies a search space of implementations
and a cost model.   



  good example: database systems. 

Possible solution: centaur-based approach.  


Somewhere we need to mention combination of static and dynamic methods
and argue for why static methods alone are likely to be insufficient.

Goals:
  - help user find programs that are correct and resource-savvy for
  relevant workloads.  ``performance auto-tuning''
  - help user write programs that are robust with respect to workload
  changes.  Devise an infrastructure that allows programmers to specify their
  intent at a high level, and then synthesize a performant
  implementation.  This approach means the system can resynthesize a
  different implementation in response to changes in workload while
  leaving the high-level logic of the program unchanged.  
  - help user find inputs on which current implementation exhibits
  poor resource utlization.  annoying/unsuable and security
  vulnerability as attackers can use these inputs to create denial of
  service attacks. 


\dpw{Explain
how one does performance debugging and what is involved
(found this:  \url{http://www.brendangregg.com/linuxperf.html})  Explain
why it is hard.  Cite evidence.
Note that the advent of big data makes this more important than ever 
\em{today}.  Small
changes in performance mean changes in power and lots of money.  Explain that
the same program or same library can have different performance results 
and require different implementations depending on the
environment.  Perhaps bring in past experience with Hancock and explain
the programmer productivity bottlenecks in that context.  Ideally some data about Hancock
might be nice. 
DARPA program -- performance attacks.
}

One way to tackle these problems effectively is to use a \emph{Centaur}-based 
approach to programming~\cite{centaur}.  The ``modern centaur'' is 
part human and part machine, and it exploits the strengths of each.
Typically, the human supplies general knowledge of the external world,
specific knowledge of the problem domain, and intuition derived from life experience.  The
computer supplies the ability to test hypotheses rapidly and reliably,
to search through large spaces rapidly.
When it comes to performance debugging, the human's role is to describe
the set of possible implementations without concern for how these implementations
perform; the machine uses its computational power to search through the space of 
implementations to find the best-performing one for the current environment.

Indeed, researchers have already developed solutions to a variety of specific problems
using this technique.  For instance, ... Representation Synthesis, AutoBahn, implicit
parallelism, super-optimization via stochastic search ...

However, each of the above systems was built from scratch --- a substantial undertaking
that can only be achieved by experts willing to build their own new languages and
compilers, or to dig into the internals of existing languages.  The goal of
this proposal is to design and implement a platform for resource-aware program
synthesis.  This platform will ask users to define:

\begin{enumerate}
\item new domain-specific abstractions for use by client programs
\item the (possibly infinite) space of implementations of each abstraction
\item a cost model
\item a search strategy to navigate the space of implementations
\item data to focus optimization
\end{enumerate}

The first 4 items need only be defined once ---they form a
\emph{resource-aware synthesis plug-in}.  The last item allows
different clients to optimize for different environements.

For example, to implement Hawkin's et al.'s representation synthesis engine, the
engineer defining the resource-aware synthesis plug-in would
specify:

\begin{enumerate}
\item an interface with functions to insert, delete and look up records in a database
\item a description of the implementations, which will be in terms of key-value maps,
including, for instance, a hash table, a list and a vector implementation
\item the cost model is simply the running time
\item the search strategy is brute force enumeration
\end{enumerate}

Then users of that module could supply example data/programs and re-optimize for
their environment.





%\begin{figure}[t]
%% \begin{wrapfigure}{R}{0.4\textwidth}
%%   \centering
%%   \includegraphics[width=.35\textwidth]{figures/errors2} \\
%%   \caption{
%% Juniper study~\cite{juniper-study}: 50-80\% of outages are the result of human error.}
%%   \label{fig:network-downtime}
%% \end{wrapfigure}
%\end{figure}




\paragraph*{Intellectual Merit.}


\paragraph{The Team.}  Our team has the breadth of skills, backgrounds, and perspectives that will be required to accomplish the agenda set out above.  


We hypothesize that resource-aware synthesis, implemented via the kind
of synthetic language extensions we propose, can improve the
performance of programs in many dimensions.  To illustrate several of
the key technical ideas underlying our proposed design, we flesh out a
simple, yet powerful concrete example in this section, based on our
prior work~\cite{autobahn}, which involves synthesis of strictness
annotations in Haskell.  After fleshing out our ideas in the context
of this application, we consider other closely related applications
including synthesis of parallel or distributed components and
incrementalization of programs.

\subsection{Autobahn}

\dpw{Ideas and text plagiarized and/or paraphrased from autobahn paper:}
Lazy functional programming languages such as
Haskell offer the promise of only evaluating the expressions needed
to compute the answer. As such, they often enable useful programming
idioms, modular designs~\cite{?} and the definition of powerful,
yet syntactically-lightweight, first-class control constructs. 
However, laziness does not always improve performance---quite the
opposite.  Laziness is implemented using thunks: When a function is called, the
system passes a heap-allocated thunk storing an unevaluated argument to
the function. If in the execution of the function it is determined that
the value of the argument is actually needed, the thunk is forced,
which causes the argument to be evaluated to weak head normal
form. The thunk is then overwritten with the resulting value so future
references donâ€™t need to re-evaluate it.  Unfortunately,
allocating thunks that always eventually need to be forced is expensive,
particularly if those thunks retain pointers to large data structures
that are otherwise unneeded in the future.  The retention of such
pointers causes space leaks, because the garbage collector cannot
reclaim the space until the thunk is evaluated.

To deal with this problem, Haskell allows users to add
\emph{strictness annotations} to their programs.  These annotations
cause immediate evaluation of computations. Oftentimes, only a few
annotations need to be added, but figuring out where to put them often
involves profiling, trial and error, insight and experience.  For a
while, experts thought this problem could be solved the problem by
placing annotations in Haskell libraries, written by
experts. Unfortunately, this approach cannot work because the
annotations needed within the library code depends upon how the
library is used, which changes from one program to the next, and is
not something the library writer can decide on in advance.

In recent work, PI Fisher developed a new system called Autobahn~\cite{autobahn}
to infer strictness annotations for Haskell programs.  Autobahn
operates by assuming that strictness annotations may (or may not) be
needed at every possible point in a program, or, alternatively, within
user-directed regions of a program.  Next, given sample data with which to
execute the program, Autobahn searches for the set of strictness annotations
that optimizes (run-time) performance of the application on the sample data.
Given the vast number of possible program annotations, the search space is 
large.  However, our research has discovered that genetic programming finds
effective set of annotations in most cases.

While Autobahn is an effective new tool for optimizing Haskell's evaluation
order, it was costly to construct.  In particular, the researchers
required knowledge of Haskell's compiler internals and had to generate
infrastructure to generate variations of user programs, execute them,
measure performance and search for optimal variations.  All this
infrastructure and knowledge was deployed to build just one tool.  
However, Autobahn is just one of many --- there are other proposals to
search for implicit parallelism~\cite{?}, to automatically incrementalize
programs~\cite{type-directed-incrementalization}, to use stochastic search
in super-optimization~\cite{?,?,?}, and to find optimal data 
representations~\cite{?}.  The goal of our research proposal
is to make it easier to build and adapt optimizing extensions like
Autobahn and others, and to allow ordinary programmers, rather than 
compiler experts to do so.  

\subsection{Autobahn via \rsynth}
\label{sec:autobahn}

Figure~\ref{fig:autobahn-via-synthesis} sketches an example of a synthetic 
language plug-in designed to implement Autobahn and Figure~\ref{fig:autobahn-client} sketches a client program that uses this language plug-in.  These two 
figures present
several technical elements of proposed design:  \emph{typed symbolic values}, 
\emph{symbolic computations}, \emph{search strategies}, language directives to
\emph{synthesize} optimal code from example data, language directives to
find possible \emph{performance vulnerabilities}. 

\begin{figure}[t]
    %% \centering
    %% \begin{minipage}{.5\textwidth}
    %%     \centering
    %%     \includegraphics[width=0.6\textwidth]{figures/datacenter-topo}
    %%     \caption{Data Center Topology}
    %%     \label{fig:data-center-topo}
    %% \end{minipage}%
    %%\begin{minipage}{0.5\textwidth}
        
\centering
\begin{mylisting}
#lang Autobahn

-- define symbolic values
symbolic gen* :: bool

-- define search strategy for symbolic values
strategy gen* = 
  genetic { diversityRate = 0.4
            , numGenerations = 20
            , populationSize = 15
            , archiveSize = 7
            , mutateRate = 0.2
            , mutateProb = 0.2
            , crossRate = 0.8
            , numFitnessRuns = 4 }

-- define search space using symbolic values
eval? :: 'a -> 'a
eval? e =
  if gen* then
    !e
  else
    e

-- define syntax extension, which uses eval? to eager evaluate appropriate
-- function applications
syntax extension ...
\end{mylisting}
\caption{Autobahn via resource-aware program synthesis.  
\dpw{Kathleen, I would like to make this look syntactically
as though it is a sort of Haskell language extension so that
if a Haskell person reviews the paper, they will node their head 
and say ``yes this looks like a plausible way to extend haskell with
components of this form.  We also need to do something plausible with
the syntax extension, which I left blank.  We could do the syntax
using template haskell.  A nice argument to make however is that users
don't need to know compiler internals to work with what we are doing.
Can ``?'' be a part of an identifier in Haskell?
}
}
\label{fig:autobahn-via-synthesis}
%%\end{minipage}
\end{figure}

\begin{figure}[t]
    %% \centering
    %% \begin{minipage}{.5\textwidth}
    %%     \centering
    %%     \includegraphics[width=0.6\textwidth]{figures/datacenter-topo}
    %%     \caption{Data Center Topology}
    %%     \label{fig:data-center-topo}
    %% \end{minipage}%
    %%\begin{minipage}{0.5\textwidth}
        
\centering
\begin{mylisting}
#lang Autobahn    -- use language extensions

-- application code
component x = ...

-- generate optimal program using Autobahn search strategy
-- example data component 0, ... component n
#synthesize [component 0, component 1, ... component n]

-- generate possible performance vulnerabilities for 
#vulnerabilities component
\end{mylisting}
\caption{Autobahn client program.}
\label{fig:autobahn-client}
%%\end{minipage}
\end{figure}

\paragraph*{Symbolic values}

\paragraph*{Strategies}

\paragraph*{Synthesis}

\paragraph*{Performance Vulnerabilities}



\section{Representation synthesis via \rsynth}
\label{sec:eval}

\section{Evaluation}
\label{sec:eval}


\section{Broader Impacts}
\label{sec:impact}


\section{Results from Prior NSF Support}
\label{sec:prior-support}

\noindent
{\bf David Walker, PI. NSF CNS-1111520, Intellectual Merit:}
In NSF CNS-1111520, \emph{High-Level Language Support for Trustworthy Networks}
(\$1,400,000, 08/11-07/16),
PI Walker and his collaborators developed new languages, interfaces
and systems for managing software-defined networks (SDNs).  
This project produced the Frenetic family
of network programming languages, the first high-level languages for
programming software-defined networks.  These languages, which include
Frenetic~\cite{frenetic}, 
Pyretic~\cite{pyretic},
NetKAT~\cite{netkat} and others, all adhere to the
\emph{principle of compositionality}, a key design element missing
from earlier network programming languages.  
%They also invented the
%notion of consistent network update~\cite{reitblatt+:consistent-updates},
%which ensures key safety invariants are preserved across network update.
Open source code for systems produced by this project is available
at \url{frenetic-lang.org}.
%
{\bf Broader Impacts:} 
The PIs held a well-attended summer school on network programming and 
verification for students and faculty. The
Pyretic programming language was used in Nick Feamster's popular
SDN MOOC; thousands of students and
network operators all over the US used it to learn principles of network
programming.  The PIs
also helped create the P4 switch configuration language~\cite{P4}, which is
becoming an industry standard.

\medskip
\noindent
{\bf Kathleen Fisher} 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "proposal.tex"
%%% TeX-PDF-mode: t
%%% End:
