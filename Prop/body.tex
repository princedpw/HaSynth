
\dpw{Explain
how one does performance debugging and what is involved
(found this:  \url{http://www.brendangregg.com/linuxperf.html})  Explain
why it is hard.  Cite evidence.
Note that the advent of big data makes this more important than ever 
\em{today}.  Small
changes in performance mean changes in power and lots of money.  
Perhaps bring in past experience with Hancock and explain
the programmer productivity bottlenecks in that context.  Ideally some data about Hancock
might be nice. 
DARPA program -- performance attacks.
}



\section{Introduction}
When writing robust software, functional correctness is only the
beginning.  
High-quality code also needs to make savvy use of resources.  
In constructing modern software systems, 
programmers must make many implementation decisions, but
the best choices depend upon a complex array of factors that may be
only partially understandable even to excellent programmers.
Examples of such factors include the difficult-to-predict effects of 
concurrency control and parallelization,
the shifting demands of dynamic workloads,  
jit compilers, 
implementations of lazy language features,
garbage collection,
hardware and/or software caches, and
distributing computations across a variety of machines or data
centers.
%
Even if programmers were able to obtain acceptable performance on a
particular hardware configuration with a particular compiler/runtime 
on a particular set of workloads, such codes are unlikely to
be robust as the hardware/software/workload system evolves.
%
Library writers face particuarly daunting challenges as the
appropriate implementation choices may depend crucially on information
not available until the library is used.  Exposing different API calls
for different workloads, for example as Haskell's JSON parsing library
does, is a brittle and clumsy mechanism for performance tuning.

One way to tackle these problems is to use a
\emph{Centaur}-based approach~\cite{centaur}.
The ``modern centaur'' is part human and part machine, and it exploits
the strengths of each.  Typically, the human supplies general
knowledge of the external world, specific knowledge of the problem
domains, and intuition derived from life experience.
The computer supplies the ability to test hypotheses efficiently and
reliably and to search through large spaces rapidly.  
In such a partnership targetting resource-savvy programming, 
the human's role is to describe the set of
possible implementations without concern for how those implementations
perform and a set of representative workloads; 
the machine uses its computational power to search through
the space of implementations to find the one(s) with the best performance.
The machine makes use of dynamic measurements on the representative
workloads to cope with the fact that purely analytic approaches are
unlikely to be able to faithfully model resource utilization.

Researchers have already developed solutions to a variety of specific problems
using this technique.  For instance, ... Representation Synthesis, AutoBahn, implicit
parallelism, super-optimization via stochastic search ...
Indeed, database systems can be seen as another example of this
approach.

However, each of the above systems was built from scratch --- a substantial undertaking
that can only be achieved by experts willing to build their own new languages and
compilers, or to dig into the internals of existing languages.  The goal of
this proposal is to design and implement a platform for resource-savvy
synthesis.  This platform will ask users to define:

\begin{enumerate}
\item new domain-specific abstractions for use by client programs
\item the (possibly infinite) space of implementations of each abstraction
\item a cost model
\item a search strategy to navigate the space of implementations, and 
\item a test harness including representative inputs.
\end{enumerate}
%
The first four items need only be defined once ---they form a
\emph{resource-aware synthesis plug-in}, what we call a ``\rasp{}.''
The last item allows clients to instantiate the \rasp{} differently for
different contexts and workloads.

For example, to implement Hawkin's et al.'s representation synthesis engine, the
engineer defining the \rasp{} would specify:

\begin{enumerate}
\item an interface with functions to insert, delete, and look up records in a relational table
\item a description of the legal implementations, which will be in terms of key-value maps
(such as hash tables, lists, and vectors), allowable sharing
relationships, and functional dependencies
\item the cost model, which is simply the running time on the supplied test harness
\item the search strategy, which is brute force enumeration of the implementation space
\end{enumerate}
%
Users of the \rasp{} could supply different harnesses to re-optimize for
different use cases.

The \rasp{} platform will help end users in a variety of domains to
\begin{enumerate}
\item find programs that are functionally correct and resource-savvy for
  relevant workloads.  This part of the effort can be seen as producing
  a general framework for autotuning.
\item write programs that are robust with respect to workload and/or
  environmental 
  changes.  Because our framework will allow programmers to specify their
  intent at a high level and then synthesize a performant
  implementation, the system can resynthesize a
  different implementation in response to changes in workload or
  infrastructure 
  \textit{while leaving the high-level logic of the program unchanged}.  
\item find outlier inputs on which the current implementation exhibits
  poor resource utilization.  Because the system makes use of dynamic
  measurement on a supplied workload, it can produce implementations
  that give poor performance on other inputs.  Such behavior may be
  acceptable or even desired, for example in cases where the humans know that
  those inputs are impossible and that ruling them out leads to
  significant performance improvements.  However, it may be that the
  supplied workload was not sufficiently broad and the outlier needs
  to be considered.  Failing to do so can lead to annoying performance
  for end users and exposes the system to denial of service attacks
  by hackers.   By identifying outliers, the system allows users to
  either filter them out as illegal input or add them to the
  representative workload the system considers when searching for
  performant implementations.
\end{enumerate}


%\begin{figure}[t]
%% \begin{wrapfigure}{R}{0.4\textwidth}
%%   \centering
%%   \includegraphics[width=.35\textwidth]{figures/errors2} \\
%%   \caption{
%% Juniper study~\cite{juniper-study}: 50-80\% of outages are the result of human error.}
%%   \label{fig:network-downtime}
%% \end{wrapfigure}
%\end{figure}




\paragraph*{Intellectual Merit.}


\paragraph{The Team.}  Our team has the breadth of skills, backgrounds, and perspectives that will be required to accomplish the agenda set out above.  


We hypothesize that resource-aware synthesis, implemented via the kind
of synthetic language extensions we propose, can improve the
performance of programs in many dimensions.  To illustrate several of
the key technical ideas underlying our proposed design, we flesh out a
simple, yet powerful concrete example in this section, based on our
prior work~\cite{autobahn}, which involves synthesis of strictness
annotations in Haskell.  After fleshing out our ideas in the context
of this application, we consider other closely related applications
including synthesis of parallel or distributed components and
incrementalization of programs.

\subsection{Autobahn}

\dpw{Ideas and text plagiarized and/or paraphrased from autobahn paper:}
Lazy functional programming languages such as
Haskell offer the promise of only evaluating the expressions needed
to compute the answer. As such, they often enable useful programming
idioms, modular designs~\cite{?} and the definition of powerful,
yet syntactically-lightweight, first-class control constructs. 
However, laziness does not always improve performance---quite the
opposite.  Laziness is implemented using thunks: When a function is called, the
system passes a heap-allocated thunk storing an unevaluated argument to
the function. If in the execution of the function it is determined that
the value of the argument is actually needed, the thunk is forced,
which causes the argument to be evaluated to weak head normal
form. The thunk is then overwritten with the resulting value so future
references donâ€™t need to re-evaluate it.  Unfortunately,
allocating thunks that always eventually need to be forced is expensive,
particularly if those thunks retain pointers to large data structures
that are otherwise unneeded in the future.  The retention of such
pointers causes space leaks, because the garbage collector cannot
reclaim the space until the thunk is evaluated.

To deal with this problem, Haskell allows users to add
\emph{strictness annotations} to their programs.  These annotations
cause immediate evaluation of computations. Oftentimes, only a few
annotations need to be added, but figuring out where to put them often
involves profiling, trial and error, insight and experience.  For a
while, experts thought this problem could be solved the problem by
placing annotations in Haskell libraries, written by
experts. Unfortunately, this approach cannot work because the
annotations needed within the library code depends upon how the
library is used, which changes from one program to the next, and is
not something the library writer can decide on in advance.

In recent work, PI Fisher developed a new system called Autobahn~\cite{autobahn}
to infer strictness annotations for Haskell programs.  Autobahn
operates by assuming that strictness annotations may (or may not) be
needed at every possible point in a program, or, alternatively, within
user-directed regions of a program.  Next, given sample data with which to
execute the program, Autobahn searches for the set of strictness annotations
that optimizes (run-time) performance of the application on the sample data.
Given the vast number of possible program annotations, the search space is 
large.  However, our research has discovered that genetic programming finds
effective set of annotations in most cases.

While Autobahn is an effective new tool for optimizing Haskell's evaluation
order, it was costly to construct.  In particular, the researchers
required knowledge of Haskell's compiler internals and had to generate
infrastructure to generate variations of user programs, execute them,
measure performance and search for optimal variations.  All this
infrastructure and knowledge was deployed to build just one tool.  
However, Autobahn is just one of many --- there are other proposals to
search for implicit parallelism~\cite{?}, to automatically incrementalize
programs~\cite{type-directed-incrementalization}, to use stochastic search
in super-optimization~\cite{?,?,?}, and to find optimal data 
representations~\cite{?}.  The goal of our research proposal
is to make it easier to build and adapt optimizing extensions like
Autobahn and others, and to allow ordinary programmers, rather than 
compiler experts to do so.  

\subsection{Autobahn via \rsynth}
\label{sec:autobahn}

Figure~\ref{fig:autobahn-via-synthesis} sketches an example of a synthetic 
language plug-in designed to implement Autobahn and Figure~\ref{fig:autobahn-client} sketches a client program that uses this language plug-in.  These two 
figures present
several technical elements of proposed design:  \emph{typed symbolic values}, 
\emph{symbolic computations}, \emph{search strategies}, language directives to
\emph{synthesize} optimal code from example data, language directives to
find possible \emph{performance vulnerabilities}. 

\begin{figure}[t]
    %% \centering
    %% \begin{minipage}{.5\textwidth}
    %%     \centering
    %%     \includegraphics[width=0.6\textwidth]{figures/datacenter-topo}
    %%     \caption{Data Center Topology}
    %%     \label{fig:data-center-topo}
    %% \end{minipage}%
    %%\begin{minipage}{0.5\textwidth}
        
\centering
\begin{mylisting}
{-# LANGUAGE Synthesis DEFINING Autobahn -}

-- a vector of symbolic booleans -- one per occurrence of gen* in the source code
symbolic gen* :: bool

-- define the space of possible implementations
eval? e =
  if gen* then
    !e
  else
    e

-- rewrite syntax to use eval? at each functional application
map\_syntax (\Apply(e1,e2) -> Apply(e1, Apply-inline (eval?, e2)))

-- define the search strategy for the vector gen*
{-# STRATEGY gen* = 
              genetic { diversityRate = 0.4
                      , numGenerations = 20
                      , populationSize = 15
                      , archiveSize = 7
                      , mutateRate = 0.2
                      , mutateProb = 0.2
                      , crossRate = 0.8
                      , numFitnessRuns = 4 }
-}
\end{mylisting}
\caption{Autobahn via resource-aware program synthesis.  
\dpw{Kathleen, I would like to make this look syntactically
as though it is a sort of Haskell language extension so that
if a Haskell person reviews the paper, they will node their head 
and say ``yes this looks like a plausible way to extend haskell with
components of this form.  We also need to do something plausible with
the syntax extension, which I left blank.  We could do the syntax
using template haskell.  A nice argument to make however is that users
don't need to know compiler internals to work with what we are doing.
Can ``?'' be a part of an identifier in Haskell?
}
}
\label{fig:autobahn-via-synthesis}
%%\end{minipage}
\end{figure}

\begin{figure}[t]
    %% \centering
    %% \begin{minipage}{.5\textwidth}
    %%     \centering
    %%     \includegraphics[width=0.6\textwidth]{figures/datacenter-topo}
    %%     \caption{Data Center Topology}
    %%     \label{fig:data-center-topo}
    %% \end{minipage}%
    %%\begin{minipage}{0.5\textwidth}
        
\centering
\begin{mylisting}
{-# LANGUAGE Autobahn -}    -- use the Autobahn extension

-- ordinary application code;  function applications may or may not be strict, as synthesized
component x = ...

-- generate optimal program using example data component 0, ..., component n
{-# SYNTHESIZE "mycomponent" [component 0, component 1, ..., component n] "dir/code" -}

-- generate performance vulnerabilities report
{-# VULNERABILITIES "mycomponent" "report.txt" -}
\end{mylisting}
\caption{Autobahn client program.}
\label{fig:autobahn-client}
%%\end{minipage}
\end{figure}

\paragraph*{Symbolic values}

\paragraph*{Strategies}

\paragraph*{Synthesis}

\paragraph*{Performance Vulnerabilities}



\section{Representation synthesis via \rsynth}
\label{sec:eval}

\section{Evaluation}
\label{sec:eval}


\section{Broader Impacts}
\label{sec:impact}


\section{Results from Prior NSF Support}
\label{sec:prior-support}

\noindent
{\bf David Walker, PI. NSF CNS-1111520, Intellectual Merit:}
In NSF CNS-1111520, \emph{High-Level Language Support for Trustworthy Networks}
(\$1,400,000, 08/11-07/16),
PI Walker and his collaborators developed new languages, interfaces
and systems for managing software-defined networks (SDNs).  
This project produced the Frenetic family
of network programming languages, the first high-level languages for
programming software-defined networks.  These languages, which include
Frenetic~\cite{frenetic}, 
Pyretic~\cite{pyretic},
NetKAT~\cite{netkat} and others, all adhere to the
\emph{principle of compositionality}, a key design element missing
from earlier network programming languages.  
%They also invented the
%notion of consistent network update~\cite{reitblatt+:consistent-updates},
%which ensures key safety invariants are preserved across network update.
Open source code for systems produced by this project is available
at \url{frenetic-lang.org}.
%
{\bf Broader Impacts:} 
The PIs held a well-attended summer school on network programming and 
verification for students and faculty. The
Pyretic programming language was used in Nick Feamster's popular
SDN MOOC; thousands of students and
network operators all over the US used it to learn principles of network
programming.  The PIs
also helped create the P4 switch configuration language~\cite{P4}, which is
becoming an industry standard.

\medskip
\noindent
{\bf Kathleen Fisher} 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "proposal.tex"
%%% TeX-PDF-mode: t
%%% End:
